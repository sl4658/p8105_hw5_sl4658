---
title: "p8105_hw5_sl4658"
author: "Simin Ling"
date: "11/17/2020"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(purrr)
library(rvest)
```

## Problem 1
Load and describe the raw data.
```{r}
homicide_raw = read_csv("./homicide-data.csv") 

head(homicide_raw)
```

There are `r nrow(homicide_raw)` rows/observations and `r ncol(homicide_raw)` columns in the raw data. The `homicide_raw` data includes information on the homicide report date and location, the victim name, age and other demographic information, as well as whether an arrest was made. Each row corresponds to a homicide case being recorded.


Create a `city_state` variable and a `resolved` variable that represents the disposition of homicide case
```{r}
homicide_df =
  homicide_raw %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolved = case_when (
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved",
    )
  ) %>%
  select(city_state, resolved) %>%
  filter(city_state != "Tulsa_AL")
```
Note: We drop Tulsa AL, because there is only one homicide case recorded for this city.


Summarize within cities on the total number of homicides and the total number of unsolved homicides
```{r}
aggregate_df =  
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )

aggregate_df
```


For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved, apply `broom::tidy` to this object, and pull the estimated proportion and CIs from the resulting tidy dataframe.
```{r}
baltimore_df = 
  prop.test(
    aggregate_df %>% filter(city_state == "Baltimore, MD") %>% pull(hom_unsolved), 
    aggregate_df %>% filter(city_state == "Baltimore, MD") %>% pull(hom_total)
  ) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)

baltimore_df
```


Run `prop.test` for each city in the dataset `aggregate_df`, and extract both the proportion of unsolved homicides and the confidence interval for each. 
```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)

results_df
```


Create a plot that shows the estimates and CIs for each city 
```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(x = "City, State", y = "Estimated proportion of unsolved homicides", title = "Estimated proportions and CIs of unsolved homicides for each city")
```



## Problem 2
Import and tidy the data
```{r}
data_1 = read_csv("lda_data/con_01.csv")
```

```{r, error = TRUE}
path_df = 
  tibble(
    path = list.files("lda_data")) %>% 
  mutate(
    path = str_c("lda_data/", path),
    data = map(path, read_csv)) %>%
  unnest(cols = data) %>%
  separate(path, into = c("file", "arm", "id"), sep = "_") %>% 
  select(-file) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation") %>%
  mutate(
    arm = str_replace(arm, "lda_data/", ""),
    arm = recode(arm, "con" = "control",
                      "exp" = "experiment"),
    id = as.character(id),
    id = as.numeric(str_replace(id, ".csv", "")),
    week = as.numeric(week, ".csv", ""))
```

Create a spaghetti plot showing observations on each subject over time
```{r}
spaghetti = 
  path_df %>%
  ggplot(aes(x = week, y = observation, group = id, color = arm)) + 
  geom_line() + 
  facet_grid(. ~arm) +
  labs(
    x = "Week",
    y = "Observation",
    title = "Observations on each subject over time"
  )

spaghetti
```

As shown in the graph, the observations on subjects in both control and experiment groups started from a relatively close value at week 0. The experiment group experienced an increasing trend in observation across time, while the control group remained relatively stable over the study time.



## Problem 3
Write a simulation function to explore power in a one-sample test (Fix n=30, sigma=5)
```{r}
sim_mean_sd = function(mu) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = 5),
  ) %>%
    t.test(mu = 0, conf.level = 0.95) %>%
    broom::tidy() %>%
    select(estimate, p.value) %>%
    rename(c("mu_hat" = "estimate", "p_value" = "p.value"))
}
```
  

Set mu=0. Generate 5000 datasets from the model 
```{r}
sim_results = 
  rerun(5000, sim_mean_sd(mu = 0) %>%
          bind_rows()
  )
```


Repeat the model for mu={1,2,3,4,5,6}
```{r}
sim_results =
  tibble(mu = 1:6) %>%
  mutate(
    sim_output = map(.x = mu, ~rerun(5000, sim_mean_sd(mu = .x))),
    sim_estimate = map(sim_output, bind_rows)
  ) %>%
  select(-sim_output) %>%
  unnest(sim_estimate)
```


Make a plot showing the proportion of times the null was rejected on the y axis and the true value of μ on the x axis
```{r}
plot_1 = 
  sim_results %>% 
  mutate(
    conclusion = if_else(p_value < 0.05, "reject", "fail to reject"),
    mu = as.factor(mu)
  ) %>%
  group_by(mu) %>%
  summarize(
    reject = sum(conclusion == "reject"),
    total = n(),
    proportion = reject/total*100
    ) %>% 
  ggplot(aes(x = mu, y = proportion)) +
  geom_point() +
  labs(
    title = "Power of the test over the true mean of the test",
    x = "True value of mean",
    y = "Power (proportion of times the null was rejected)"
  )

plot_1
```

As shown on the plot, we can see that the power of test increased with the increase in effect size. This indicates a positive correlation between power of test and effect size. Power of test reached its highest point around effect size of 4 and remained steady after that. 



Make a plot `plot_2` showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. 
```{r}
plot_2 =
  sim_results %>%
  mutate(mu = as.factor(mu)) %>%
  group_by(mu) %>%
  summarize(mean_mu = mean(mu_hat)) %>%
  ggplot(aes(x = mu, y = mean_mu)) +
  geom_point(color = "red") +
  ylim(0,7) +
  labs(
    title = "Average estimate of mu over the true value of mu in the entire data",
    x = "True value of mu",
    y = "Average estimate of mu") 

plot_2
```

Make another plot `plot_3` with the average estimate of μ̂  only in samples for which the null was rejected.
```{r}
plot_3 = 
  sim_results %>%
  filter(p_value < 0.05) %>%
  mutate(mu = as.factor(mu)) %>%
  group_by(mu) %>%
  summarize(mean_mu = mean(mu_hat)) %>%
  ggplot(aes(x = mu, y = mean_mu)) +
  geom_point(color = "orange") +
  ylim(0,7) +
  labs(
    title = "Average estimate of mu over the true value of mu in the rejected data",
    x = "True value of mu",
    y = "Average estimate of mu")

plot_3
```


For lower values of true mu (1 and 2), the sample average of `mu_hat` across samples for which the null was rejected is not equal to the true mean, instead it's larger than the true mean. However, as the true mu gets larger, the sample average of `mu_hat` across samples for which the null was rejected becomes close to the true mean for true mean.  

This may be explained by the increase in power with the increase in effect size. When we have a constant sample size, a greater effect size will give us more precise result that is close to the truth.
